{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features Selected: ['mosque_count_500', 'full_sq', 'culture_objects_top_25_raion', 'cafe_count_5000_price_high', 'school_km', 'cafe_count_3000_price_2500', 'cafe_count_1000_price_4000', 'cafe_count_5000_price_2500', 'cafe_count_3000_price_high', 'public_transport_station_min_walk', 'sport_count_3000', 'cafe_count_3000', 'cafe_count_1500', 'leisure_count_500', 'additional_education_km', 'cafe_count_1000', 'exhibition_km', 'build_count_monolith', 'ts_km', 'park_km', 'big_church_km', 'nuclear_reactor_km', 'incineration_km', 'ID_big_road2', 'office_sqm_3000', 'railroad_station_avto_km', 'stadium_km', 'cafe_count_2000', '16_29_male', 'raion_build_count_with_builddate_info']\n",
      "Best submission saved as submissions/best_submission_xgb_1_rmse11635870.0381_est500_depth7_lr0.01_sub1.0_col0.7.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop(columns=[\"price_doc\", \"id\"], errors='ignore')\n",
    "y = train[\"price_doc\"]\n",
    "\n",
    "# Handle Outliers (Example: Remove extreme price outliers)\n",
    "q1 = y.quantile(0.01)\n",
    "q99 = y.quantile(0.99)\n",
    "outlier_mask = (y >= q1) & (y <= q99)\n",
    "X = X[outlier_mask]\n",
    "y = y[outlier_mask]\n",
    "\n",
    "# Impute missing values\n",
    "numerical_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Numerical imputation\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "X[numerical_features] = num_imputer.fit_transform(X[numerical_features])\n",
    "test[numerical_features] = num_imputer.transform(test[numerical_features])\n",
    "\n",
    "# Categorical encoding and imputation\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_features] = cat_imputer.fit_transform(X[categorical_features])\n",
    "test[categorical_features] = cat_imputer.transform(test[categorical_features])\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "test[numerical_features] = scaler.transform(test[numerical_features])\n",
    "\n",
    "# XGBoost for Feature Selection\n",
    "xgb_fs_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    n_estimators=100  # Use a moderate number of trees for feature selection\n",
    ")\n",
    "xgb_fs_model.fit(X, y)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importances = pd.Series(xgb_fs_model.feature_importances_, index=X.columns)\n",
    "top_features = feature_importances.nlargest(30).index  # Select top 30 features\n",
    "print(\"Top Features Selected:\", top_features.tolist())\n",
    "\n",
    "# Keep only the top features\n",
    "X = X[top_features]\n",
    "test = test[top_features]\n",
    "\n",
    "# Train-Test Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost Regressor for Training\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Expanded Hyperparameter Grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "}\n",
    "\n",
    "# Track the best models\n",
    "best_models = []\n",
    "best_rmse = float('inf')\n",
    "\n",
    "# Prepare Submission Directory\n",
    "if not os.path.exists(\"submissions\"):\n",
    "    os.makedirs(\"submissions\")\n",
    "\n",
    "# Grid Search Loop\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for learning_rate in param_grid['learning_rate']:\n",
    "            for subsample in param_grid['subsample']:\n",
    "                for colsample_bytree in param_grid['colsample_bytree']:\n",
    "                    params = {\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_depth': max_depth,\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'subsample': subsample,\n",
    "                        'colsample_bytree': colsample_bytree,\n",
    "                    }\n",
    "                    model = xgb.XGBRegressor(\n",
    "                        **params,\n",
    "                        objective='reg:squarederror',\n",
    "                        tree_method='hist',\n",
    "                        n_jobs=-1,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                    # Validate\n",
    "                    val_predictions = model.predict(X_val)\n",
    "                    val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "\n",
    "                    # Check if this is the best model\n",
    "                    if val_rmse < best_rmse:\n",
    "                        best_rmse = val_rmse\n",
    "                        best_models = [(model, params, val_rmse)]\n",
    "                    elif val_rmse == best_rmse:  # Handle ties\n",
    "                        best_models.append((model, params, val_rmse))\n",
    "\n",
    "# Generate Submission Files for Best Models\n",
    "for idx, (best_model, params, rmse) in enumerate(best_models):\n",
    "    # Predict on Test Data\n",
    "    test_predictions = best_model.predict(test)\n",
    "\n",
    "    # Save Submission File\n",
    "    submission = pd.DataFrame({\n",
    "        \"row ID\": test.index.map(lambda x: f\"Row{x}\"),\n",
    "        \"price_doc\": test_predictions\n",
    "    })\n",
    "    param_info = f\"est{params['n_estimators']}_depth{params['max_depth']}_lr{params['learning_rate']}_sub{params['subsample']}_col{params['colsample_bytree']}\"\n",
    "    submission_file = f\"submissions/best_submission_xgb_{idx+1}_rmse{rmse:.4f}_{param_info}.csv\"\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"Best submission saved as {submission_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
